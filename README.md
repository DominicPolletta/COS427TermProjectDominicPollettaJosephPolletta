# COS427TermProjectDominicPollettaJosephPolletta
Term Project for USM Fall 2021 Computational Text Analytics for Dominic Polletta and Joseph Polletta

Team Members: Dominic Polletta and Joseph Polletta

Objective for the project: The objective is to classify abstract texts of medical publications obtained from the PubMed database to test the different machine learning models on real world data. We will obtain a large amount of journal articles from PubMed, extract the abstracts from each article to be used as a Train/Test set for Naive Bayes, Support Vector Machine, and Logistic Regression machine learning models. Using the same test data, we will inspect the different qualities of each model, evaluating their benefits and downsides, both on their own and when compared to other machine learning models used.

Materials: We will be utilizing a set of abstracts pulled from articles we will obtain from the PubMed database for four different diseases, Acute Rheumatic Arthritis, Cardiovascular Abnormalities, Lyme Disease, and Knee Osteoarthritis.

Methods: We will be normalizing the abstracts, then converting them into a standardized Pandas DataFrame, which we will then use as the input for three different types of machine learning models. The models we will be using are Naive Bayes, Support Vector Machine (SVM), Logistic Regression.

Experimental Validation: We obtained the accuracy, precision, recall, and f1 scores by comparing the true positives, true negatibes, false positives, and false negatives generated by our models after feeding them new data generated from the train_test_split that it had not seen before. We then the predicted classification from the data and then was tested against the actual classification for the data. In order to ensure the validity of our results, we used the same machine to test the run times, the same data fed into each model after the same preprocessing methods, and each train_test_split was ran with the same random shuffle for consistent results. This ensures that all of our results are experimentally valid.

Results: 
Multionomial Naive Bayes:
Confusion Matrix:
{{1556   91   24   39}
 {  64 2975   19    5}
 {   8    6 3038    4}
 {  33   16    6 2364}}
 
Accuracy Score (%): 96.92622950819673%
Precision Score (%): 96.61045729003105%
Recall Score (%): 96.3146199200599%
F1 Score (%): 96.45658063256386%
 
One Versus Rest SVM:
Confusion Matrix:
{{1593   80   16   21}
 {  89 2942   22   10}
 {  16   30 3005    5}
 {  26   19    9 2365}}
 
Accuracy Score (%): 96.65300546448088%
Precision Score (%): 96.29036423377088%
Recall Score (%): 96.32658592809418%
F1 Score (%): 96.30769607365662%

One Versus One SVM:
Confusion Matrix:
{{1589   83   18   20}
 {  88 2941   22   12}
 {  13   27 3012    4}
 {  31   18    6 2364}}
 
Accuracy Score (%): 96.66275346604216%
Precision Score (%): 96.2887624687193%
Recall Score (%): 96.3068740119793%
F1 Score (%): 96.29717332606785%

Logistic Regression:
Confusion Matrix:
{{1600   77   12   21}
 {  66 2976   12    9}
 {  12   23 3016    5}
 {  26   21    5 2367}}
 
Accuracy Score (%): 97.17993754879001%
Precision Score (%): 96.89505170361079
Recall Score (%): 96.8170843195081%
F1 Score (%): 96.85487566524067%

Conclusion: We found that Logistic Regression was the most accurate machine learning model for our dataset, but also had the longest run time by far. At this scale of dataset, the increased time cost is managable but at a larger dataset it is possible that the longer time to run may not be worth it compared to another model.

Discussion: One aspect of the project that we could attempt in the future to improve accuracy would involve the way that we break down the data for analysis. In this project we utilized a homemade Bag of Words algorithm to serve as the data input to our machine learning models. However, Bag of Words does not take into account the order of the words. If we were to use a different method, such as Word2Vec, which takes into account the order of the words, then it may be possible to further improve our results. Another worry that occured when accumulating the data was the unbalanced nature of the classes. At the start, we were worried that the fact that two of our classes were at 10,000 examples while our other two close to a 1/4 of that number would impact the accuracy significantly. We were prepared to manually adjust weighting of the data to ensure that the classes were handled in a balanced manner. However, we did not see any signs of this being an issue with our results. We hypothesize that the SKLearn package was able to hangle the inbalanced classes, as there were enough in each to ensure that there wasn't issues with too small a dataset.

Outlook: We now have a baseline reading for different machine learning models on medical journal abstracts. There several avenues for expanding our efforts, such as attempting the same tests but with different types of texts such as social media posts. Another avenue would be to use Word2Vec instead of Bag Of Words, or to attempt using more types of machine learning models to further evaluate.
